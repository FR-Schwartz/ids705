{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Kaggle Competition and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Fides Regina Schwartz*\n",
    "Netid: fs113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), and is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html).\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "Through completing this assignment you will be able to...\n",
    "1. Apply the full supervised machine learning pipeline of preprocessing, model selection, model performance evaluation and comparison, and model application to a real-world scale dataset\n",
    "2. Apply clustering techniques to a variety of datasets with diverse distributional properties, gaining an understanding of their strengths and weaknesses and how to tune model parameters\n",
    "3. Apply PCA and t-SNE for performing dimensionality reduction and data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "## [40 points] Kaggle Classification Competition\n",
    "\n",
    "You've learned a great deal about supervised learning and now it's time to bring together all that you've learned. You will be competing in a Kaggle Competition along with the rest of the class! Your goal is to predict hotel reservation cancellations based on a number of potentially related factors such as lead time on the booking, time of year, type of room, special requests made, number of children, etc. While you will be asked to take certain steps along the way to your submission, you're encouraged to try creative solutions to this problem and your choices are wide open for you to make your decisions on how to best make the predictions.\n",
    "\n",
    "### IMPORTANT: Follow the link posted on Ed to register for the competition\n",
    "You can view the public leaderboard anytime [here](https://www.kaggle.com/c/ids705-a5-2022/leaderboard)\n",
    "\n",
    "**The Data**. The dataset is provided as `a5_q1.pkl` which is a pickle file format, which allows you to load the data directly using the code below; the data can be downloaded from the [Kaggle competition website](https://www.kaggle.com/c/ids705-a5-2022/data). A data dictionary for the project can be found [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md) and the original paper that describes the dataset can be found [here](https://www.sciencedirect.com/science/article/pii/S2352340918315191). When you load the data, 5 matrices are provided `X_train_original`, `y_train`, and `X_test_original`, which are the original, unprocessed features and labels for the training set and the test features (the test labels are not provided - that's what you're predicting). Additionally, `X_train_ohe` and `X_test_ohe` are provided which are one-hot-encoded (OHE) versions of the data. The OHE versions OHE processed every categorical variable. This is provided for convenience if you find it helpful, but you're welcome to reprocess the original data other ways if your prefer.\n",
    "\n",
    "**Scoring**. You will need to achieve a minimum acceptable level of performance to demonstrate proficiency with using these supervised learning techniques. Beyond that, it's an open competition and scoring in the top three places of the *private leaderboard* will result in **5 bonus points in this assignment** (and the pride of the class!). Note: the Kaggle leaderboard has a public and private component. The public component is viewable throughout the competition, but the private leaderboard is revealed at the end. When you make a submission, you immediately see your submission on the public leaderboard, but that only represents scoring on a fraction of the total collection of test data, the rest remains hidden until the end of the competition to prevent overfitting to the test data through repeated submissions. You will be be allowed to hand-select two eligible submissions for private score, or by default your best two public scoring submissions will be selected for private scoring.\n",
    "\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**(a) Explore your data.** Review and understand your data. Look at it; read up on what the features represent; think through the application domain; visualize statistics from the paper data to understand any key relationships. **There is no output required for this question**, but you are encouraged to explore the data personally before going further.\n",
    "\n",
    "**(b) Preprocess your data.** Preprocess your data so it's ready for use for classification and describe what you did and why you did it. Preprocessing may include: normalizing data, handling missing or erroneous values, separating out a validation dataset, preparing categorical variables through one-hot-encoding, etc. To make one step in this process easier, you're provided with a one-hot-encoded version of the data already. \n",
    "- Comment on each type of preprocessing that you apply and both how and why you apply it.\n",
    "\n",
    "**(c) Select, train, and compare models.** Fit at least 5 models to the data. Some of these can be experiments with different hyperparameter-tuned versions of the same model, although all 5 should not be the same type of model. There are no constraints on the types of models, but you're encouraged to explore examples we've discussed in class including:\n",
    "\n",
    "1. Logistic regression\n",
    "2. K-nearest neighbors\n",
    "3. Random Forests\n",
    "4. Neural networks\n",
    "5. Support Vector Machines\n",
    "6. Ensembles of models (e.g. model bagging, boosting, or stacking). `Scikit-learn` offers a number of tools for assisting with this including those for [bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier), [boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and [stacking](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html). You're also welcome to explore options beyond the `sklean` universe; for example, some of you may have heard of [XGBoost](https://github.com/dmlc/xgboost) which is a very fast implementation of gradient boosted decision trees that also allows for parallelization. \n",
    "\n",
    "When selecting models, be aware that some models may take far longer than others to train. Monitor your output and plan your time accordingly. \n",
    "\n",
    "Assess the classification performance AND computational efficiency of the models you selected:\n",
    "- Plot the ROC curves and PR curves for your models in two plots: one of ROC curves and one of PR curves. For each of these two plots, compare the performance of the models you selected above and trained on the training data, evaluating them on the validation data. Be sure to plot the line representing random guessing on each plot. One of these models should also be your BEST performing submission on the Kaggle public leaderboard (see below). In the legends of each, include the area under the curve for each model (limit to 3 significant figures). For the ROC curve, this is the AUC; for the PR curve, this is the average precision (AP).\n",
    "- As you train and validate each model time how long it takes to train and validate in each case and create a plot that shows both the training and prediction time for each model included in the ROC and PR curves.\n",
    "- Describe: \n",
    "  - Your process of model selection and hyperparameter tuning\n",
    "  - Which model performed best and your process for identifying/selecting it\n",
    "\n",
    "**(d) Apply your model \"in practice\".** Make *at least* 5 submissions of different model results to the competition (more submissions are encouraged and you can submit up to 10 per day!). These do not need to be the same that you report on above, but you should select your *most competitive* models.\n",
    "- Produce submissions by applying your model on the test data.\n",
    "- Be sure to RETRAIN YOUR MODEL ON ALL LABELED TRAINING AND VALIDATION DATA before making your predictions on the test data for submission. This will help to maximize your performance on the test data.\n",
    "- In order to get full credit on this problem you must achieve an AUC on the Kaggle public leaderboard above the \"Benchmark\" score on the public leaderboard.\n",
    "\n",
    "### Guidance:\n",
    "1. **Preprocessing**. You may need to preprocess the data for some of these models to perform well (scaling inputs or reducing dimensionality). Some of this preprocessing may differ from model to model to achieve the best performance. A helpful tool for creating such preprocessing and model fitting pipelines is the sklearn `pipeline` module which lets you group a series of processing steps together.\n",
    "2. **Hyperparameters**. Hyperparameters may need to be tuned for some of the model you use. You may want to perform hyperparameter tuning for some of the models. If you experiment with different hyperparameters that include many model runs, you may want to apply them to a small subsample of your overall data before running it on the larger training set to be time efficient (if you do, just make sure to ensure your selected subset is representative of the rest of your data).\n",
    "3. **Validation data**. You're encouraged to create your own validation dataset for comparing model performance; without this, there's a significant likelihood of overfitting to the data. A common choice of the split is 80% training, 20% validation. Before you make your final predictions on the test data, be sure to retrain your model on the entire dataset.\n",
    "4. **Training time**. This is a larger dataset than you've worked with previously in this class, so training times may be higher that what you've experienced in the past. Plan ahead and get your model pipeline working early so you can experiment with the models you use for this problem and have time to let them run. \n",
    "\n",
    "### Starter code\n",
    "Below is some code for (1) loading the data and (2) once you have predictions in the form of confidence scores for those classifiers, to produce submission files for Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Explore your data.** Review and understand your data. Look at it; read up on what the features represent; think through the application domain; visualize statistics from the paper data to understand any key relationships. **There is no output required for this question**, but you are encouraged to explore the data personally before going further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "data = pickle.load( open( \"C:/Users/dm93/Desktop/IDS705/ids705-a5-2022/a5_q1.pkl\", \"rb\" ) )\n",
    "\n",
    "y_train = data['y_train']\n",
    "X_train_original = data['X_train'] # Original dataset\n",
    "X_train_ohe = data['X_train_ohe']  # One-hot-encoded dataset\n",
    "\n",
    "X_test_original = data['X_test']\n",
    "X_test_ohe = data['X_test_ohe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95512, 940)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95512,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23878, 940)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total observations from the paper describe 119,390 datapoints and that is the amount of data contained in the training and test datasets in total, too. This confirms that I have the full dataset available to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_date_year</th>\n",
       "      <th>arrival_date_week_number</th>\n",
       "      <th>arrival_date_day_of_month</th>\n",
       "      <th>stays_in_weekend_nights</th>\n",
       "      <th>stays_in_week_nights</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>babies</th>\n",
       "      <th>is_repeated_guest</th>\n",
       "      <th>...</th>\n",
       "      <th>company_530.0</th>\n",
       "      <th>company_531.0</th>\n",
       "      <th>company_534.0</th>\n",
       "      <th>company_539.0</th>\n",
       "      <th>company_541.0</th>\n",
       "      <th>company_543.0</th>\n",
       "      <th>customer_type_Contract</th>\n",
       "      <th>customer_type_Group</th>\n",
       "      <th>customer_type_Transient</th>\n",
       "      <th>customer_type_Transient-Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>342</td>\n",
       "      <td>2015</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 940 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lead_time  arrival_date_year  arrival_date_week_number  \\\n",
       "0        342               2015                        27   \n",
       "2          7               2015                        27   \n",
       "3         13               2015                        27   \n",
       "4         14               2015                        27   \n",
       "5         14               2015                        27   \n",
       "\n",
       "   arrival_date_day_of_month  stays_in_weekend_nights  stays_in_week_nights  \\\n",
       "0                          1                        0                     0   \n",
       "2                          1                        0                     1   \n",
       "3                          1                        0                     1   \n",
       "4                          1                        0                     2   \n",
       "5                          1                        0                     2   \n",
       "\n",
       "   adults  children  babies  is_repeated_guest  ...  company_530.0  \\\n",
       "0       2       0.0       0                  0  ...              0   \n",
       "2       1       0.0       0                  0  ...              0   \n",
       "3       1       0.0       0                  0  ...              0   \n",
       "4       2       0.0       0                  0  ...              0   \n",
       "5       2       0.0       0                  0  ...              0   \n",
       "\n",
       "   company_531.0  company_534.0  company_539.0  company_541.0  company_543.0  \\\n",
       "0              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "5              0              0              0              0              0   \n",
       "\n",
       "   customer_type_Contract  customer_type_Group  customer_type_Transient  \\\n",
       "0                       0                    0                        1   \n",
       "2                       0                    0                        1   \n",
       "3                       0                    0                        1   \n",
       "4                       0                    0                        1   \n",
       "5                       0                    0                        1   \n",
       "\n",
       "   customer_type_Transient-Party  \n",
       "0                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "5                              0  \n",
       "\n",
       "[5 rows x 940 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_date_year</th>\n",
       "      <th>arrival_date_week_number</th>\n",
       "      <th>arrival_date_day_of_month</th>\n",
       "      <th>stays_in_weekend_nights</th>\n",
       "      <th>stays_in_week_nights</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>babies</th>\n",
       "      <th>is_repeated_guest</th>\n",
       "      <th>...</th>\n",
       "      <th>company_530.0</th>\n",
       "      <th>company_531.0</th>\n",
       "      <th>company_534.0</th>\n",
       "      <th>company_539.0</th>\n",
       "      <th>company_541.0</th>\n",
       "      <th>company_543.0</th>\n",
       "      <th>customer_type_Contract</th>\n",
       "      <th>customer_type_Group</th>\n",
       "      <th>customer_type_Transient</th>\n",
       "      <th>customer_type_Transient-Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95510.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "      <td>95512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>103.849768</td>\n",
       "      <td>2016.157205</td>\n",
       "      <td>27.152902</td>\n",
       "      <td>15.823038</td>\n",
       "      <td>0.928491</td>\n",
       "      <td>2.503288</td>\n",
       "      <td>1.855746</td>\n",
       "      <td>0.103696</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>0.031598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.034017</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.750419</td>\n",
       "      <td>0.210759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>106.722804</td>\n",
       "      <td>0.707470</td>\n",
       "      <td>13.611204</td>\n",
       "      <td>8.786777</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>1.918017</td>\n",
       "      <td>0.596925</td>\n",
       "      <td>0.397763</td>\n",
       "      <td>0.093348</td>\n",
       "      <td>0.174929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.181273</td>\n",
       "      <td>0.069157</td>\n",
       "      <td>0.432773</td>\n",
       "      <td>0.407850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>69.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>709.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 940 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lead_time  arrival_date_year  arrival_date_week_number  \\\n",
       "count  95512.000000       95512.000000              95512.000000   \n",
       "mean     103.849768        2016.157205                 27.152902   \n",
       "std      106.722804           0.707470                 13.611204   \n",
       "min        0.000000        2015.000000                  1.000000   \n",
       "25%       18.000000        2016.000000                 16.000000   \n",
       "50%       69.000000        2016.000000                 27.000000   \n",
       "75%      160.000000        2017.000000                 38.000000   \n",
       "max      709.000000        2017.000000                 53.000000   \n",
       "\n",
       "       arrival_date_day_of_month  stays_in_weekend_nights  \\\n",
       "count               95512.000000             95512.000000   \n",
       "mean                   15.823038                 0.928491   \n",
       "std                     8.786777                 0.999940   \n",
       "min                     1.000000                 0.000000   \n",
       "25%                     8.000000                 0.000000   \n",
       "50%                    16.000000                 1.000000   \n",
       "75%                    23.000000                 2.000000   \n",
       "max                    31.000000                19.000000   \n",
       "\n",
       "       stays_in_week_nights        adults      children        babies  \\\n",
       "count          95512.000000  95512.000000  95510.000000  95512.000000   \n",
       "mean               2.503288      1.855746      0.103696      0.007748   \n",
       "std                1.918017      0.596925      0.397763      0.093348   \n",
       "min                0.000000      0.000000      0.000000      0.000000   \n",
       "25%                1.000000      2.000000      0.000000      0.000000   \n",
       "50%                2.000000      2.000000      0.000000      0.000000   \n",
       "75%                3.000000      2.000000      0.000000      0.000000   \n",
       "max               50.000000     55.000000      3.000000      9.000000   \n",
       "\n",
       "       is_repeated_guest  ...  company_530.0  company_531.0  company_534.0  \\\n",
       "count       95512.000000  ...   95512.000000   95512.000000   95512.000000   \n",
       "mean            0.031598  ...       0.000052       0.000010       0.000010   \n",
       "std             0.174929  ...       0.007235       0.003236       0.003236   \n",
       "min             0.000000  ...       0.000000       0.000000       0.000000   \n",
       "25%             0.000000  ...       0.000000       0.000000       0.000000   \n",
       "50%             0.000000  ...       0.000000       0.000000       0.000000   \n",
       "75%             0.000000  ...       0.000000       0.000000       0.000000   \n",
       "max             1.000000  ...       1.000000       1.000000       1.000000   \n",
       "\n",
       "       company_539.0  company_541.0  company_543.0  customer_type_Contract  \\\n",
       "count   95512.000000   95512.000000   95512.000000            95512.000000   \n",
       "mean        0.000010       0.000010       0.000021                0.034017   \n",
       "std         0.003236       0.003236       0.004576                0.181273   \n",
       "min         0.000000       0.000000       0.000000                0.000000   \n",
       "25%         0.000000       0.000000       0.000000                0.000000   \n",
       "50%         0.000000       0.000000       0.000000                0.000000   \n",
       "75%         0.000000       0.000000       0.000000                0.000000   \n",
       "max         1.000000       1.000000       1.000000                1.000000   \n",
       "\n",
       "       customer_type_Group  customer_type_Transient  \\\n",
       "count         95512.000000             95512.000000   \n",
       "mean              0.004806                 0.750419   \n",
       "std               0.069157                 0.432773   \n",
       "min               0.000000                 0.000000   \n",
       "25%               0.000000                 1.000000   \n",
       "50%               0.000000                 1.000000   \n",
       "75%               0.000000                 1.000000   \n",
       "max               1.000000                 1.000000   \n",
       "\n",
       "       customer_type_Transient-Party  \n",
       "count                   95512.000000  \n",
       "mean                        0.210759  \n",
       "std                         0.407850  \n",
       "min                         0.000000  \n",
       "25%                         0.000000  \n",
       "50%                         0.000000  \n",
       "75%                         0.000000  \n",
       "max                         1.000000  \n",
       "\n",
       "[8 rows x 940 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at summary statistics\n",
    "X_train_ohe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95512 entries, 0 to 119389\n",
      "Columns: 940 entries, lead_time to customer_type_Transient-Party\n",
      "dtypes: float64(2), int64(15), uint8(923)\n",
      "memory usage: 97.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import pycountry as pc\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "ax = sns.heatmap(X_train_ohe, linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER (a)**\n",
    "After reviewing the data, I believe the important factors that might predict cancellations in Portugese hotels (Algarve vs Lisbon) to be:\n",
    "1. lead time (less lead time might be correlated with less likelihood of cancellation, because people have less time to change their plans) \n",
    "2. time of booking (separated into spring (March-May), summer (June-August), autumn (September-November) and winter (December-February), assuming that winter bookings may be canceled more frequently because inclement weather might hinder travel) \n",
    "3. number of children and babies combined (more children might increase likelihood of cancellation)\n",
    "4. Length of stay combining weekends and weekdays (assuming that shorter stays might get canceled more frequently, because they are less of a committment)\n",
    "5. Weekend vs Weekday stays (assuming that weekend stays are more likely to be booked as freetime activities and might get canceled more often due to changing plans, whereas weeknights are more often booked by business travelers or during longer term stays which do not get canceled as often)\n",
    "6. Whether stays were booked in the resort or city hotel (assuming that the resort receives more people on holiday and the city receives more business travel with the associated implications for cancellation)\n",
    "7. Meal (assuming that someone who booked a stay with full board would be less likely to cancel)\n",
    "8. Country (assuming that travelers from some countries are more \"reliable\" and cancel fewer bookings than travelers from other countries)\n",
    "8. Market segment (assuming that tour operators book more in bulk and may not be able to fill all their slots vs travel agents who only buy by request, so may have to cancel fewer bookings)\n",
    "9. Repeat guest (assuming that people who have stayed in the same location before are less likely to cancel)\n",
    "10. Previous cancellations (assuming that people with more previous cancellations are at higher risk of cancelling again)\n",
    "11. Booking changes (assuming that more changes to the booking might increase the risk of cancellation)\n",
    "12. Deposit (assuming that a non-refundable deposit decreases the likelihood of cancellation)\n",
    "13. Agent and company (assuming that some agents and/or companies are more \"reliable\" and make fewer cancellations than others)\n",
    "14. Days in waiting list (assuming that the longer it takes for booking confirmation, the more likely a cancellation will occur)\n",
    "15. Customer type (assuming that bookings associated with a group or contract are less likely to be canceled)\n",
    "16. Average daily rate (assuming that higher daily rates are more likely to be canceled)\n",
    "17. Required car parking spaces (assuming that the more parking spaces requested, the less likely to cancel)\n",
    "18. Number of special requests (assuming that the higher the number of special requests made, the more likely a guest is to cancel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Preprocess your data.** Preprocess your data so it's ready for use for classification and describe what you did and why you did it. Preprocessing may include: normalizing data, handling missing or erroneous values, separating out a validation dataset, preparing categorical variables through one-hot-encoding, etc. To make one step in this process easier, you're provided with a one-hot-encoded version of the data already. \n",
    "- Comment on each type of preprocessing that you apply and both how and why you apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the company columns\n",
    "#function to join columns if column is not null\n",
    "def sjoin(x): return ';'.join(x[x.notnull()].astype(int))\n",
    "\n",
    "#function to ignore the suffix on the column e.g. company_530, company_531 will be grouped together\n",
    "def groupby_field(col):\n",
    "    parts = col.split('company_')\n",
    "    return '{}'.format(parts[0])\n",
    "\n",
    "df = X_train_ohe.groupby(groupby_field, axis=1,).apply(lambda x: x.apply(sjoin, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children        2\n",
       "lead_time       0\n",
       "company_82.0    0\n",
       "company_64.0    0\n",
       "company_65.0    0\n",
       "company_67.0    0\n",
       "company_68.0    0\n",
       "company_71.0    0\n",
       "company_72.0    0\n",
       "company_73.0    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find missing values, show the total null values for each column and sort it in descending order\n",
    "X_train_ohe.isnull().sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children        2\n",
       "lead_time       0\n",
       "company_82.0    0\n",
       "company_64.0    0\n",
       "company_65.0    0\n",
       "company_67.0    0\n",
       "company_68.0    0\n",
       "company_71.0    0\n",
       "company_72.0    0\n",
       "company_73.0    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.isna().sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe['children'].fillna(round(data.children.mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two NA values in the dataset in the column \"children\". I replace these with the mean of all the other rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with NULL entries\n",
    "# If agent NULL=not applicable, drop\n",
    "X_train_ohe.drop(X_train_ohe.loc[X_train_ohe['agent']=='NULL'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for all columns that start with company\n",
    "filter_col = [col for col in X_train_ohe if col.startswith('company')]\n",
    "#filter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn companies into categoricals\n",
    "dummies = pd.get_dummies(X_train_ohe['filter_col'])\n",
    "X_train_ohe['Company'] = dummies.cumsum(axis=1).ne(1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_time                        int64\n",
       "arrival_date_year                int64\n",
       "arrival_date_week_number         int64\n",
       "arrival_date_day_of_month        int64\n",
       "stays_in_weekend_nights          int64\n",
       "                                 ...  \n",
       "company_543.0                    uint8\n",
       "customer_type_Contract           uint8\n",
       "customer_type_Group              uint8\n",
       "customer_type_Transient          uint8\n",
       "customer_type_Transient-Party    uint8\n",
       "Length: 940, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training dataset into training and validation dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ohepartial, X_val_ohe, y_train_ohepartial, y_val_ohe = train_test_split(\n",
    "    X_train_ohe, y_train, test_size = 0.2, random_state=2018\n",
    ") # split with an 80/20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohepartial.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER (b)**\n",
    "\n",
    "I did several iterations of preprocessing.\n",
    "\n",
    "1. Normalizing the data:\n",
    "\n",
    "    i) scaling\n",
    "\n",
    "    ii) reducing dimensionality\n",
    "\n",
    "2. Drop missing and erroneous values\n",
    "    i) since there was no missing data, based on the database this data was pulled from, none could be dropped directly. Entries that were presented as NULL were considered as not applicable and dropped, though.\n",
    "\n",
    "3. Prepare categorical variables\n",
    "\n",
    "4. Split training data into training and validation data using an 80/20 split. This is to evaluate performance of my models based on a held-out dataset before it gets applied to the true test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Select, train, and compare models.** Fit at least 5 models to the data. Some of these can be experiments with different hyperparameter-tuned versions of the same model, although all 5 should not be the same type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the important modules\n",
    "from pandas import read_csv # For dataframes\n",
    "from pandas import DataFrame # For dataframes\n",
    "from numpy import ravel # For matrices\n",
    "import matplotlib.pyplot as plt # For plotting data\n",
    "import seaborn as sns # For plotting data\n",
    "from sklearn.model_selection import train_test_split # For train/test splits\n",
    "from sklearn.linear_model import LogisticRegression # The logistic regression classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # The k-nearest neighbor classifier\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
    "# Various pre-processing steps\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV # For optimization\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tuning pipeline for logistic regression\n",
    "# set up tuning pipleine for KNN\n",
    "logreg = LogisticRegression().fit(X_train_ohe, y_train)\n",
    "print('Training set score: ' + str(logreg.score(X_train_ohepartial,y_train_ohepartial)))\n",
    "print('Validation set score: ' + str(logreg.score(X_val_ohe,y_val_ohe)))\n",
    " \n",
    "pipe = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('reduce_dim', PCA()),\n",
    "('regressor', Ridge())\n",
    "])\n",
    " \n",
    "pipe.fit(X_train_ohepartial, y_train_ohepartial)\n",
    " \n",
    "print('Training set score: ' + str(pipe.score(X_train_ohepartial,y_train_ohepartial)))\n",
    "print('Validation set score: ' + str(pipe.score(X_val_ohe,y_val_ohe)))\n",
    " \n",
    "parameters = {'scaler': [StandardScaler(), MinMaxScaler(),\n",
    "\tNormalizer(), MaxAbsScaler()],\n",
    "\t'selector__threshold': [0, 0.001, 0.01],\n",
    "\t'classifier__n_neighbors': [1, 3, 5, 7, 10],\n",
    "\t'classifier__p': [1, 2],\n",
    "\t'classifier__leaf_size': [1, 5, 10, 15]\n",
    "}\n",
    " \n",
    "grid = GridSearchCV(pipe, parameters, cv=2).fit(X_train_ohepartial, y_train_ohepartial)\n",
    " \n",
    "print('Training set score: ' + str(grid.score(X_train_ohepartial, y_train_ohepartial)))\n",
    "print('Validation set score: ' + str(grid.score(X_val_ohe, y_val_ohe)))\n",
    " \n",
    "# Access the best set of parameters\n",
    "best_params = grid.best_params_\n",
    "print(best_params)\n",
    "# Stores the optimum model in best_pipe\n",
    "best_pipe = grid.best_estimator_\n",
    "print(best_pipe)\n",
    " \n",
    "result_df = DataFrame.from_dict(grid.cv_results_, orient='columns')\n",
    "print(result_df.columns)\n",
    " \n",
    "sns.relplot(data=result_df,\n",
    "\tkind='line',\n",
    "\tx='param_classifier__n_neighbors',\n",
    "\ty='mean_test_score',\n",
    "\thue='param_scaler',\n",
    "\tcol='param_classifier__p')\n",
    "plt.show()\n",
    " \n",
    "sns.relplot(data=result_df,\n",
    "            kind='line',\n",
    "            x='param_classifier__n_neighbors',\n",
    "            y='mean_test_score',\n",
    "            hue='param_scaler',\n",
    "            col='param_classifier__leaf_size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - logistic regression\n",
    "# Time the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model - logistic regression\n",
    "# Time model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tuning pipleine for KNN\n",
    "knn = KNeighborsClassifier().fit(X_train_ohe, y_train)\n",
    "print('Training set score: ' + str(knn.score(X_train_ohepartial,y_train_ohepartial)))\n",
    "print('Validation set score: ' + str(knn.score(X_val_ohe,y_val_ohe)))\n",
    " \n",
    "pipe = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('selector', VarianceThreshold()),\n",
    "('classifier', KNeighborsClassifier())\n",
    "])\n",
    " \n",
    "pipe.fit(X_train_ohepartial, y_train_ohepartial)\n",
    " \n",
    "print('Training set score: ' + str(pipe.score(X_train_ohepartial,y_train_ohepartial)))\n",
    "print('Validation set score: ' + str(pipe.score(X_val_ohe,y_val_ohe)))\n",
    " \n",
    "parameters = {'scaler': [StandardScaler(), MinMaxScaler(),\n",
    "\tNormalizer(), MaxAbsScaler()],\n",
    "\t'selector__threshold': [0, 0.001, 0.01],\n",
    "\t'classifier__n_neighbors': [1, 3, 5, 7, 10],\n",
    "\t'classifier__p': [1, 2],\n",
    "\t'classifier__leaf_size': [1, 5, 10, 15]\n",
    "}\n",
    " \n",
    "grid = GridSearchCV(pipe, parameters, cv=2).fit(X_train_ohepartial, y_train_ohepartial)\n",
    " \n",
    "print('Training set score: ' + str(grid.score(X_train_ohepartial, y_train_ohepartial)))\n",
    "print('Validation set score: ' + str(grid.score(X_val_ohe, y_val_ohe)))\n",
    " \n",
    "# Access the best set of parameters\n",
    "best_params = grid.best_params_\n",
    "print(best_params)\n",
    "# Stores the optimum model in best_pipe\n",
    "best_pipe = grid.best_estimator_\n",
    "print(best_pipe)\n",
    " \n",
    "result_df = DataFrame.from_dict(grid.cv_results_, orient='columns')\n",
    "print(result_df.columns)\n",
    " \n",
    "sns.relplot(data=result_df,\n",
    "\tkind='line',\n",
    "\tx='param_classifier__n_neighbors',\n",
    "\ty='mean_test_score',\n",
    "\thue='param_scaler',\n",
    "\tcol='param_classifier__p')\n",
    "plt.show()\n",
    " \n",
    "sns.relplot(data=result_df,\n",
    "            kind='line',\n",
    "            x='param_classifier__n_neighbors',\n",
    "            y='mean_test_score',\n",
    "            hue='param_scaler',\n",
    "            col='param_classifier__leaf_size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - KNN\n",
    "# Time the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model - KNN\n",
    "# Time model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - RF\n",
    "# Time the model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model -RF \n",
    "# Time model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - NN\n",
    "# Time the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model - NN\n",
    "# Time model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - SVM\n",
    "# Time the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model - SVM\n",
    "# Time model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Ensembles of models (e.g. model bagging, boosting, or stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                          LinearSVC(random_state=2018)))\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    ")\n",
    "clf.fit(X_train_ohepartial, y_train_ohepartial).score(X_val_ohe, y_val_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - stacking\n",
    "# Time the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model - stacking\n",
    "# Time model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC and PR curves for the three best models - Including: Random guessing, AUC and AP + training and prediction time for full training dataset (train + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model_lr = LogisticRegression(penalty='l1', C=1e100, solver=\"liblinear\")\n",
    "model_lr_lasso = LogisticRegression(penalty='l1', C=1e-2, solver=\"liblinear\")\n",
    "model_rf = RandomForestClassifier()\n",
    "model_lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "models = [model_lr, model_lr_lasso, model_lda, model_rf]\n",
    "labels = ['Logistic Regression','Logistic Regression + LASSO', 'Linear Discriminant Analysis','Random Forest']\n",
    "\n",
    "# Initialize each plot\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "# Plot the chance diagonal and PR random chance lines\n",
    "axs[0].plot((0,1),(0,1),color='lightgrey',linestyle='--', label='Chance')\n",
    "axs[1].plot((0,1),(fraction_positive_examples, fraction_positive_examples),color='lightgrey',linestyle='--', label='Chance')\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'Training model: {labels[i]}')\n",
    "    # Fit the model to the data\n",
    "    model.fit(X_train_ohe, y_train)\n",
    "    \n",
    "    # Produce confidence scores from the test data\n",
    "    scores = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Compute the ROC and PR curve data\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, scores)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    axs[0].plot(fpr, tpr, label=labels[i] + ', AUC = {:.3f}'.format(roc_auc))\n",
    "    \n",
    "    # Plot the PR curve\n",
    "    axs[1].plot(recall, precision, label=labels[i])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('square')\n",
    "    ax.set(xlim=(0,1), ylim=(0,1))\n",
    "    ax.legend()\n",
    "axs[0].set(xlabel='False Positive Rate', ylabel='True Positive Rate', title='ROC Curve')\n",
    "axs[1].set(xlabel='Recall', ylabel='Precision', title='Precision Recall Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the classification performance AND computational efficiency of the models you selected:\n",
    "- Plot the ROC curves and PR curves for your models in two plots: one of ROC curves and one of PR curves. For each of these two plots, compare the performance of the models you selected above and trained on the training data, evaluating them on the validation data. Be sure to plot the line representing random guessing on each plot. One of these models should also be your BEST performing submission on the Kaggle public leaderboard (see below). In the legends of each, include the area under the curve for each model (limit to 3 significant figures). For the ROC curve, this is the AUC; for the PR curve, this is the average precision (AP).\n",
    "- As you train and validate each model time how long it takes to train and validate in each case and create a plot that shows both the training and prediction time for each model included in the ROC and PR curves.\n",
    "- Describe: \n",
    "  - Your process of model selection and hyperparameter tuning\n",
    "  - Which model performed best and your process for identifying/selecting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER (c)**\n",
    "*Model selection and hyperparameter tuning*\n",
    "I tested five different models ...\n",
    "*Which model performed best, how did I identify/select it*\n",
    "I evaluated each model's AUC and PR in addition to the time it took to train and validate its performance. I selected the model with the highest AUC and best PR, which was ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Apply your model \"in practice\".** Make *at least* 5 submissions of different model results to the competition (more submissions are encouraged and you can submit up to 10 per day!). These do not need to be the same that you report on above, but you should select your *most competitive* models.\n",
    "- Produce submissions by applying your model on the test data.\n",
    "- Be sure to RETRAIN YOUR MODEL ON ALL LABELED TRAINING AND VALIDATION DATA before making your predictions on the test data for submission. This will help to maximize your performance on the test data.\n",
    "- In order to get full credit on this problem you must achieve an AUC on the Kaggle public leaderboard above the \"Benchmark\" score (**0.94933**) on the public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model on all training and validation data i.e. X_train_ohe (pre-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Produce submission\n",
    "################################\n",
    "\n",
    "def create_submission(confidence_scores, save_path):\n",
    "    '''Creates an output file of submissions for Kaggle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confidence_scores : list or numpy array\n",
    "        Confidence scores (from predict_proba methods from classifiers) or\n",
    "        binary predictions (only recommended in cases when predict_proba is \n",
    "        not available)\n",
    "    save_path : string\n",
    "        File path for where to save the submission file.\n",
    "    \n",
    "    Example:\n",
    "    create_submission(my_confidence_scores, './data/submission.csv')\n",
    "\n",
    "    '''\n",
    "    import pandas as pd\n",
    "\n",
    "    submission = pd.DataFrame({\"score\":confidence_scores})\n",
    "    submission.to_csv(save_path, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER (d)**\n",
    "\n",
    "My final submission was for a model with a score of: , which was the ... model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## [25 points] Clustering\n",
    "\n",
    "Clustering can be used to reveal structure between samples of data and assign group membership to similar groups of samples. This exercise will provide you with experience applying clustering algorithms and comparing these techniques on various datasets to experience the pros and cons of these approaches when the structure of the data being clustered varies. For this exercise, we'll explore clustering in two dimensions to make the results more tangible, but in practice these approaches can be applied to any number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Run K-means and choose the number of clusters**. Five datasets are provided for you below and the code to load them below. \n",
    "- Scatterplot each dataset\n",
    "- For each dataset run the k-means algorithm for values of $k$ ranging from 1 to 10 and for each plot the \"elbow curve\" where you plot dissimilarity in each case. Here, you can measure dissimilarity using the within-cluster sum-of-squares, which in sklean is know as \"inertia\" and can be accessed through the `inertia_` attribute of a fit KMeans class instance.\n",
    "- For each datasets, where is the elbow in the curve of within-cluster sum-of-squares and why? Is the elbow always clearly visible? When its not clear, you will have to use your judgement in terms of selecting a reasonable number of clusters for the data. *There are also other metrics you can use to explore to measure the quality of cluster fit (but do not have to for this assignment) including the silhouette score, the Calinski-Harabasz index, and the Davies-Bouldin, to name a few within sklearn alone. However, assessing quality of fit without \"preferred\" cluster assignments to compare against (that is, in a truly unsupervised manner) is challenging because measuring cluster fit quality is typically poorly-defined and doesn't generalize across all types of inter- and intra-cluster variation.*\n",
    "- Plot your clustered data (different color for each cluster assignment) for your best $k$-means fit determined from both the elbow curve and your judgement for each dataset and your inspection of the dataset.\n",
    "\n",
    "**(b) Apply DBSCAN**. Vary the `eps` and `min_samples` parameters to get as close as you can to having the same number of clusters as your choices with K-means. In this case, the black points are points that were not assigned to clusters.\n",
    "\n",
    "**(c) Apply Spectral Clustering**. Select the same number of clusters as selected by k-means.\n",
    "\n",
    "**(d) Comment on the strengths and weaknesses of each approach**. In particular, mention: \n",
    "- Which technique worked \"best\" and \"worst\" (as defined by matching how human intuition would cluster the data) on each dataset?\n",
    "- How much effort was required to get good clustering for each method (how much parameter tuning needed to be done)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: for these clustering plots in this question, do NOT include legends indicating cluster assignment; instead just make sure the cluster assignments are clear from the plot (e.g. different colors for each cluster)*\n",
    "\n",
    "Code is provided below for loading the datasets and for making plots with the clusters as distinct colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Create / load the datasets:\n",
    "n_samples = 1500\n",
    "X0, _ = make_blobs(n_samples=n_samples, centers=2, n_features=2, random_state=0)\n",
    "X1, _ = make_blobs(n_samples=n_samples, centers=5, n_features=2, random_state=0)\n",
    "\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1.3)\n",
    "transformation = [[0.6, -0.6], [-0.2, 0.8]]\n",
    "X2 = np.dot(X, transformation)\n",
    "X3, _ = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\n",
    "X4, _ = make_moons(n_samples=n_samples, noise=.12)\n",
    "\n",
    "X = [X0, X1, X2, X3, X4]\n",
    "# The datasets are X[i], where i ranges from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Code to plot clusters\n",
    "################################\n",
    "def plot_cluster(ax, data, cluster_assignments):\n",
    "    '''Plot two-dimensional data clusters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib axis\n",
    "        Axis to plot on\n",
    "    data : list or numpy array of size [N x 2] \n",
    "        Clustered data\n",
    "    cluster_assignments : list or numpy array [N]\n",
    "        Cluster assignments for each point in data\n",
    "\n",
    "    '''\n",
    "    clusters = np.unique(cluster_assignments)\n",
    "    n_clusters = len(clusters)\n",
    "    for ca in clusters:\n",
    "        kwargs = {}\n",
    "        if ca == -1:\n",
    "            # if samples are not assigned to a cluster (have a cluster assignment of -1, color them gray)\n",
    "            kwargs = {'color':'gray'}\n",
    "            n_clusters = n_clusters - 1\n",
    "        ax.scatter(data[cluster_assignments==ca, 0], data[cluster_assignments==ca, 1],s=5,alpha=0.5, **kwargs)\n",
    "        ax.set_xlabel('feature 1')\n",
    "        ax.set_ylabel('feature 2')\n",
    "        ax.set_title(f'No. Clusters = {n_clusters}')\n",
    "        ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "## [25 points] Dimensionality reduction and visualization of digits with PCA and t-SNE\n",
    "\n",
    "**(a)** Reduce the dimensionality of the data with PCA for data visualization. Load the `scikit-learn` digits dataset (code provided to do this below). Apply PCA and reduce the data (with the associated cluster labels 0-9) into a 2-dimensional space. Plot the data with labels in this two dimensional space (labels can be colors, shapes, or using the actual numbers to represent the data - definitely include a legend in your plot).\n",
    "\n",
    "**(b)** Create a plot showing the cumulative fraction of variance explained as you incorporate from $1$ through all $D$ principal components of the data (where $D$ is the dimensionality of the data). \n",
    "- What fraction of variance in the data is UNEXPLAINED by the first two principal components of the data? \n",
    "- Briefly comment on how this may impact how well-clustered the data are. \n",
    "*You can use the `explained_variance_` attribute of the PCA module in `scikit-learn` to assist with this question*\n",
    "\n",
    "**(c)** Reduce the dimensionality of the data with t-SNE for data visualization. T-distributed stochastic neighborhood embedding (t-SNE) is a nonlinear dimensionality reduction technique that is particularly adept at embedding the data into lower 2 or 3 dimensional spaces. Apply t-SNE using the `scikit-learn` implementation to the digits dataset and plot it in 2-dimensions (with associated cluster labels 0-9). You may need to adjust the parameters to get acceptable performance. You can read more about how to use t-SNE effectively [here](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "**(d)** Briefy compare/contrast the performance of these two techniques. \n",
    "- Which seemed to cluster the data best and why?\n",
    "- Notice that t-SNE doesn't have a `fit` method, but only a `fit_transform` method. Why is this? What implications does this imply for using this method?\n",
    "*Note: Remember that you typically will not have labels available in most problems.*\n",
    "\n",
    "Code is provided for loading the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Load the data\n",
    "################################\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# load dataset\n",
    "digits = datasets.load_digits()\n",
    "n_sample = digits.target.shape[0]\n",
    "n_feature = digits.images.shape[1] * digits.images.shape[2]\n",
    "X_digits = np.zeros((n_sample, n_feature))\n",
    "for i in range(n_sample):\n",
    "    X_digits[i, :] = digits.images[i, :, :].flatten()\n",
    "y_digits = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "644px",
    "left": "1473px",
    "right": "20px",
    "top": "122px",
    "width": "367px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

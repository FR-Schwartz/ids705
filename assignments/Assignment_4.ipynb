{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Fides Regina Schwartz*\n",
    "Netid: fs113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), and is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html).\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "Through completing this assignment you will be able to...\n",
    "1. Identify key hyperparameters in neural networks and how they can impact model training and fit\n",
    "2. Build, tune the parameters of, and apply feed-forward neural networks to data\n",
    "3. Implement and explain each and every part of a standard fully-connected neural network and its operation including feed-forward propagation, backpropagation, and gradient descent.\n",
    "4. Apply a standard neural network implementation and search the hyperparameter space to select optimized values.\n",
    "5. Develop a detailed understanding of the math and practical implementation considerations of neural networks, one of the most widely used machine learning tools, so that it can be leveraged for learning about other neural networks of different model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "## [45 points] Exploring and optimizing neural network hyperparameters\n",
    "Neural networks have become ubiquitous in the machine learning community, demonstrating exceptional performance over a wide range of supervised learning tasks. The benefits of these techniques come at a price of increased computational complexity and model designs with increased numbers of hyperparameters that need to be correctly set to make these techniques work. It is common that poor hyperparameter choices in neural networks result in significant decreases in model generalization performance. The goal of this exercise is to better understand some of the key hyperparameters you will encounter in practice using neural networks so that you can be better prepared to tune your model for a given application. Through this exercise, you will explore two common approaches to hyperparameter tuning a manual approach where we greedily select the best individual hyperparameter (often people will pick potentially sensible options, try them, and hope it works) as well as a random search of the hyperparameter space which as been shown to be an efficient way to achieve good hyperparameter values. \n",
    "\n",
    "To explore this, we'll be using the example data created below throughout this exercise and the various training, validation, test splits. We will select each set of hyperparameters for our greedy/manual approach and the random search using a training/validation split, then retrain on the combined training and validation data before finally evaluating our generalization performance for both our final models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the network training leads to warnings. When we know and are OK with \n",
    "#  what's causing the warning and simply don't want to see it, we can use the \n",
    "#  following code. Run this block\n",
    "#  to disable warnings\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = 'ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Create the data\n",
    "#-----------------------------------------------------------------------------\n",
    "# Data generation function to create a checkerboard-patterned dataset\n",
    "def make_data_normal_checkerboard(n, noise=0):\n",
    "    n_samples = int(n/4)\n",
    "    shift = 0.5\n",
    "    c1a = np.random.randn(n_samples,2)*noise + [-shift, shift]\n",
    "    c1b = np.random.randn(n_samples,2)*noise + [shift, -shift]\n",
    "    c0a = np.random.randn(n_samples,2)*noise + [shift, shift]\n",
    "    c0b = np.random.randn(n_samples,2)*noise + [-shift, -shift]\n",
    "    X = np.concatenate((c1a,c1b,c0a,c0b),axis=0)\n",
    "    y = np.concatenate((np.ones(2*n_samples), np.zeros(2*n_samples)))\n",
    "    \n",
    "    # Set a cutoff to the data and fill in with random uniform data:\n",
    "    cutoff = 1.25\n",
    "    indices_to_replace = np.abs(X)>cutoff\n",
    "    for index,value in enumerate(indices_to_replace.ravel()):\n",
    "        if value:\n",
    "            X.flat[index] = np.random.rand()*2.5-1.25\n",
    "    return (X,y)\n",
    "\n",
    "# Training datasets\n",
    "np.random.seed(42)\n",
    "noise = 0.45\n",
    "X_train,y_train = make_data_normal_checkerboard(500, noise=noise)\n",
    "    \n",
    "# Validation and test data\n",
    "X_val,y_val = make_data_normal_checkerboard(500, noise=noise)\n",
    "X_test,y_test = make_data_normal_checkerboard(500, noise=noise)\n",
    "\n",
    "# For RandomSeachCV, we will need to combine training and validation sets then\n",
    "#  specify which portion is training and which is validation\n",
    "# Also, for the final performance evaluation, train on all of the training AND validation data\n",
    "X_train_plus_val = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_plus_val = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Create a predefined train/test split for RandomSearchCV (to be used later)\n",
    "validation_fold = np.concatenate((-1*np.ones(len(y_train)), np.zeros(len(y_val))))\n",
    "train_val_split = PredefinedSplit(validation_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help get you started we should always begin by visualizing our training data, here's some code that does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Code to plot the sample data\n",
    "def plot_data(ax,X,y,title, limits):\n",
    "    # Select the colors to use in the plots\n",
    "    color0 = '#121619' # Dark grey\n",
    "    color1 = '#00B050' # Green\n",
    "    color_boundary='#858585'\n",
    "    \n",
    "    # Separate samples by class\n",
    "    samples0 = X[y==0]\n",
    "    samples1 = X[y==1]\n",
    "\n",
    "    ax.plot(samples0[:,0],samples0[:,1],\n",
    "        marker='o',\n",
    "        markersize=5,\n",
    "        linestyle=\"None\",\n",
    "        color=color0,\n",
    "        markeredgecolor='w',\n",
    "        markeredgewidth=0.5,\n",
    "        label='Class 0')\n",
    "    ax.plot(samples1[:,0],samples1[:,1],\n",
    "        marker='o',\n",
    "        markersize=5,\n",
    "        linestyle=\"None\",\n",
    "        color=color1,\n",
    "        markeredgecolor='w',\n",
    "        markeredgewidth=0.5,\n",
    "        label='Class 1')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(5,5))\n",
    "limits = [-1.25, 1.25, -1.25, 1.25]\n",
    "plot_data(ax, X_train, y_train, 'Training Data', limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we want to explore control the architecture of our model and how our model is fit to our data. These hyperparameters include the (a) learning rate, (b) batch size, and the (c) regularization coefficient, as well as the (d) model architecture hyperparameters (the number of layers and the number of nodes per layer). We'll explore each of these and determine an optimized configuration of the network for this problem through this exercise. For all of the settings we'll explore and just, we'll assume the following default hyperparameters for the model (we'll use scikit learn's [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.score) as our neural network model):\n",
    "- `learning_rate_init` = 0.03\n",
    "- `hidden_layer_sizes` = (30,30) (two hidden layers, each with 30 nodes)\n",
    "- `alpha` = 0 (regularization penalty)\n",
    "- `solver` = 'sgd' (stochastic gradient descent optimizer)\n",
    "- `tol` = 1e-5 (this sets the convergence tolerance)\n",
    "- `early_stopping` = False (this prevents early stopping)\n",
    "- `activation` = 'relu' (rectified linear unit)\n",
    "- `n_iter_no_change` = 1000 (this prevents early stopping)\n",
    "- `batch_size` = 50 (size of the minibatch for stochastic gradient descent)\n",
    "- `max_iter` = 500 (maximum number of epochs, which is how many times each data point will be used, not the number of gradient steps)\n",
    "\n",
    "This default setting is our initial guess of what good values may be. Notice there are many model hyperparameters in this list: any of these could potentially be options to search over. We constrain the search to those hyperparameters that are known to have a significant impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Visualize the impact of different hyperparameter choices on classifier decision boundaries.** Visualize the impact of different hyperparameter settings. Starting with the default settings above make the following changes (only change one hyperparameter at a time). For each hyperparameter value, plot the decision boundary on the training data (you will need to train the model once for each parameter value):\n",
    "1. Vary the architecture (`hidden_layer_sizes`) by changing the number of nodes per layer while keeping the number of layers constant at 2: (2,2), (5,5), (30,30). Here (X,X) means a 2-layer network with X nodes in each layer.\n",
    "2. Vary the learning rate: 0.0001, 0.01, 1\n",
    "3. Vary the regularization: 0, 1, 10\n",
    "4. Vary the batch size: 5, 50, 500\n",
    "\n",
    "As you're exploring these settings, visit this website, the [Neural Network Playground](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=20&networkShape=2,1&seed=0.89022&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=false), which will give you the chance to interactively explore the impact of each of these parameters on a similar dataset to the one we use in this exercise. The tool also allows you to adjust the learning rate, batch size, regularization coefficient, and the architecture and to see the resulting decision boundary and learning curves. You can also visualize the model's hidden node output and its weights, and it allows you to add in transformed features as well. Experiment by adding or removing hidden layers and neurons per layer and vary the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "clf.predict(X_test[:5, :])\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the sample data\n",
    "def plot_data(ax,X,y,title, limits):\n",
    "    # Select the colors to use in the plots\n",
    "    color0 = '#121619' # Dark grey\n",
    "    color1 = '#00B050' # Green\n",
    "    color_boundary='#858585'\n",
    "    \n",
    "    # Separate samples by class\n",
    "    samples0 = X[y==0]\n",
    "    samples1 = X[y==1]\n",
    "\n",
    "    ax.plot(samples0[:,0],samples0[:,1],\n",
    "        marker='o',\n",
    "        markersize=5,\n",
    "        linestyle=\"None\",\n",
    "        color=color0,\n",
    "        markeredgecolor='w',\n",
    "        markeredgewidth=0.5,\n",
    "        label='Class 0')\n",
    "    ax.plot(samples1[:,0],samples1[:,1],\n",
    "        marker='o',\n",
    "        markersize=5,\n",
    "        linestyle=\"None\",\n",
    "        color=color1,\n",
    "        markeredgecolor='w',\n",
    "        markeredgewidth=0.5,\n",
    "        label='Class 1')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(5,5))\n",
    "limits = [-1.25, 1.25, -1.25, 1.25]\n",
    "plot_data(ax, X_train, y_train, 'Training Data', limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Manual (greedy) hyperparameter tuning I: manually optimize hyperparameters that govern the learning process, one hyperparameter at a time.** Now with some insight into which settings may work better than others, let's more fully explore the performance of these different settings in the context of our validation dataset through a manual optimization process. Holding all else constant (with the default settings mentioned above), vary each of the following parameters as specified below. Train your algorithm on the training data, and evaluate the performance of your trained algorithm on the validation dataset. Here, overall accuracy is a reasonable performance metric since the classes are balanced and we don't weight one type of error as more important than the other; therefore, use the `score` method of the `MLPClassifier` for this. Create plots of accuracy vs each parameter you vary (this will result in three plots).\n",
    "1. Vary learning rate logarithmically from $10^{-5}$ to $10^{0}$ with 20 steps\n",
    "2. Vary the regularization parameter logarithmically from $10^{-8}$ to $10^2$ with 20 steps\n",
    "3. Vary the batch size over the following values: $[1,3,5,10,20,50,100,250,500]$\n",
    "\n",
    "For each of these cases:\n",
    "- Based on the results, report your optimal choices for each of these hyperparameters and why you selected them.\n",
    "- Since neural networks can be sensitive to initialization values, you may notice these plots may be a bit noisy. Consider this when selecting the optimal values of the hyperparameters. If the noise seems significant, run the fit and score procedure multiple times and report the average. Rerunning the algorithm will change the initialization and therefore the output (assuming you do not set a random seed for that algorithm).\n",
    "- Use the chosen hyperparameter values as the new default settings for section (c) and (d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Manual (greedy) hyperparameter tuning II: manually optimize hyperparameters that impact the model architecture.** Next, we want to explore the impact of the model architecture on performance and optimize its selection. This means varying two parameters at a time instead of one as above. To do this, evaluate the validation accuracy resulting from training the model using each pair of possible numbers of nodes per layer and number of layers from the lists below. We will assume that for any given configuration the number of nodes in each layer is the same (e.g. (2,2,2), which would be a 3-layer network with 2 hidden node in each layer and (25,25) are valid, but (2,5,3) is not because the number of hidden nodes varies in each layer). Use the manually optimized values for learning rate, regularization, and batch size selected from section (b). \n",
    "- Number of nodes per layer: $[1,2,3,4,5,10,15,25,30]$\n",
    "- Number of layers = $[1,2,3,4]$\n",
    "Report the accuracy of your model on the validation data. For plotting these results, use heatmaps to plot the data in two dimensions. To make the heatmaps, you can use [this code for creating heatmaps] https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html). Be sure to include the numerical values of accuracy in each grid square as shown in the linked example and label your x, y, and color axes as always. For these numerical values, round them to **2 decimal places** (due to some randomness in the training process, any further precision is not typically meaningful).\n",
    "\n",
    "- When you select your optimized parameters, be sure to keep in mind that these values may be sensitive to the data and may offer the potential to have high variance for larger models. Therefore, select the model with the highest accuracy but lowest number of total model weights (all else equal, the simpler model is preferred). \n",
    "- What do the results show? Which parameters did you select and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Manual (greedy) model selection and retraining.** Based the optimal choice of hyperparameters, train your model with your optimized hyperparameters on all the training data AND the validation data (this is provided as `X_train_plus_val` and `y_train_plus_val`). \n",
    "- Apply the trained model to the test data and report the accuracy of your final model on the test data.\n",
    "- Plot an ROC curve of your performance (plot this with the curve in part (e) on the same set of axes you use for that question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** **Automated hyperparameter search through random search**. The manual (greedy) approach (setting one or two parameters at a time holding the rest constant), provides good insights into how the neural network hyperparameters impacts model fitting for this particular training process. However, it is limited in one very problematic way: it depends heavily on a good \"default\" setting of the hyperparameters. Those were provided for you in this exercise, but are not generally know. Our manual optimization was somewhat greedy because we picked the hyperparameters one at a time rather than looking at different combinations of hyperparameters. Adopting such a pseudo-greedy approach to that manual optimization also limits our ability to more deeply search the hyperparameter space since we don't look at simultaneous changes to multiple parameters. Now we'll use a popular hyperparameter optimization tool to accomplish that: random search.\n",
    "\n",
    "Random search is an excellent example of a hyperparameter optimization search strategy that has [been shown to be more efficient](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a?ref=https://githubhelp.com) (requiring fewer training runs) than another common approach: grid search. Grid search evaluates all possible combinations of hyperparameters from lists of possible hyperparameter settings - a very computationally expensive process. Yet another attractive alternative is [Bayesian Optimization](https://arxiv.org/abs/1807.02811), which is an excellent hyperparameter optimization strategy but we will leave that to the interested reader.\n",
    "\n",
    "Our particular random search tool will be Scikit-Learn's [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV). This performs random search employing cross validation for performance evaluation (we will adjust this to ve a train/validation split). \n",
    "\n",
    "Using `RandomizedSearchCV`, train on the training data while validating on the validation data (see instructions below on how to setup the train/validation split automatically). This tool will randomly pick combinations of parameter values and test them out, returning the best combination it finds as measured by performance on the validation set. You can use [this example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py) as a template for how to do this.\n",
    "- To make this comparable to the training/validation setup used for the greedy optimization, we need to setup a training and validation split rather than use cross validation. To do this for `RandomSearchCV` we input the COMBINED training and validation dataset (`X_train_plus_val`, and `y_train_plus_val`) and we set the `cv` parameter to be the `train_val_split` variable we provided along with the dataset. This will setup the algorithm to make its assessments training just on the training data and evaluation on the validation data. Once `RandomSearchCV` completes its search, it will fit the model one more time to the combined training and validation data using the optimized parameters as we would want it to.\n",
    "- Set the number of iterations to at least 200 (you'll look at 200 random pairings of possible hyperparameters). You can go as high as you want, but it will take longer the larger the value.\n",
    "- If you run this on Colab or any system with multiple cores, set the parameter `n_jobs` to -1 to use all available cores for more efficient training through parallelization\n",
    "- You'll need to set the range or distribution of the parameters you want to sample from. Search over the same ranges as in previous problems. To tell the algorithm the ranges to search, use lists of values for candidate batch_size, since those need to be integers rather than a range; the `loguniform` `scipy` function for setting the range of the learning rate and regularization parameter, and a list of tuples for the `hidden_layer_sizes` parameter, as you used in the greedy optimization.\n",
    "- Once the model is fit, use the `best_params_` property of the fit classifier attribute to extract the optimized values of the hyperparameters and report those and compare them to what was selected through the manual, greedy optimization.\n",
    "\n",
    "For the final generalization performance assessment:\n",
    "- State the accuracy of the optimized models on the test dataset\n",
    "- Plot the ROC curve corresponding to your best model on the test dataset through greedy hyperparameter section vs the model identified through random search (these curves should be on the same set of axes for comparison). In the legend of the plot, report the AUC for each curve\n",
    "- Plot the final decision boundary for the greedy and random search-based classifiers along with the test dataset to demonstrate the shape of the final boundary\n",
    "- How did the generalization performance compare between the hyperparameters selected through the manual (greedy) search and the random search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## [45 points] Build and test your own Neural Network for classification\n",
    "\n",
    "There is no better way to understand how one of the core techniques of modern machine learning works than to build a simple version of it yourself. In this exercise you will construct and apply your own neural network classifier. You may use numpy if you wish but no other libraries.\n",
    "\n",
    "**(a)** Create a neural network class that follows the `scikit-learn` classifier convention by implementing `fit`, `predict`, and `predict_proba` methods. Your `fit` method should run backpropagation on your training data using stochastic gradient descent. Assume the activation function is a sigmoid. Choose your model architecture to have two input nodes, two hidden layers with five nodes each, and one output node.\n",
    "\n",
    "To guide you in the right direction with this problem, please find a skeleton of a neural network class below. You absolutely MAY use additional methods beyond those suggested in this template, but the methods listed below are the minimum required to implement the model cleanly.\n",
    "\n",
    "**Strategies for debugging**. One of the greatest challenges of this implementations is that there are many parts and a bug could be present in any of them. Here are some recommended tips:\n",
    "- *Development environment*. Consider using an Integrated Development Environment (IDE). I strongly recommend the use of VS Code and the Python debugging tools in that development environment.\n",
    "- *Unit tests*. You are strongly encouraged to create unit tests for most modules. Without doing this will make your code extremely difficult to bug. You can create simple examples to feed through the network to validate it is correctly computing activations and node values. Also, if you manually set the weights of the model, you can even calculate backpropagation by hand for some simple examples (admittedly, that unit test would be challenging and is optional, but a unit test is possible). \n",
    "- *Compare against a similar architecture*. You can also verify the performance of your overall neural network by comparing it against the `scikit-learn` implementation and using the same architecture and parameters as your model (your model outputs will certainly not be identical, but they should be somewhat similar for similar parameter settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Apply your neural network. \n",
    "- Create a training and validation dataset using `sklearn.datasets.make_moons(N, noise=0.20)`, where $N_{train} = 500$ and $N_{test} = 100$. \n",
    "- Train and test your model on this dataset plotting your learning curves (training and validation error for each epoch of stochastic gradient descent, where an epoch represents having trained on each of the training samples one time). \n",
    "- Tune the learning rate and number of training epochs for your model to improve performance as needed. \n",
    "- In two subplots, plot the training data on one subplot, and the validation data on the other subplot. On each plot, also plot the decision boundary from your neural network trained on the training data. \n",
    "- Report your performance on the test data with an ROC curve and compare against the `scikit-learn` `MLPClassifier` trained with the same parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Suggest two ways in which you neural network implementation could be improved: are there any options we discussed in class that were not included in your implementation that could improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_in, n_layer1, n_layer2, n_out, learning_rate=):\n",
    "        '''__init__\n",
    "        Class constructor: Initialize the parameters of the network including\n",
    "        the learning rate, layer sizes, and each of the parameters\n",
    "        of the model (weights, placeholders for activations, inputs, \n",
    "        deltas for gradients, and weight gradients). This method\n",
    "        should also initialize the weights of your model randomly\n",
    "            Input:\n",
    "                n_in:          number of inputs\n",
    "                n_layer1:      number of nodes in layer 1\n",
    "                n_layer2:      number of nodes in layer 2\n",
    "                n_out:         number of output nodes\n",
    "                learning_rate: learning rate for gradient descent\n",
    "            Output:\n",
    "                none\n",
    "        '''\n",
    "            \n",
    "    def forward_propagation(self, x):\n",
    "        '''forward_propagation\n",
    "        Takes a vector of your input data (one sample) and feeds\n",
    "        it forward through the neural network, calculating activations and\n",
    "        layer node values along the way.\n",
    "            Input:\n",
    "                x: a vector of data representing 1 sample [n_in x 1]\n",
    "            Output:\n",
    "                y_hat: a vector (or scaler of predictions) [n_out x 1]\n",
    "                (typically n_out will be 1 for binary classification)\n",
    "        '''\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        '''compute_loss\n",
    "        Computes the current loss/cost function of the neural network\n",
    "        based on the weights and the data input into this function.\n",
    "        To do so, it runs the X data through the network to generate\n",
    "        predictions, then compares it to the target variable y using\n",
    "        the cost/loss function\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "            Output:\n",
    "                loss: a scalar measure of loss/cost\n",
    "        '''\n",
    "    \n",
    "    def backpropagate(self, x, y):\n",
    "        '''backpropagate\n",
    "        Backpropagate the error from one sample determining the gradients\n",
    "        with respect to each of the weights in the network. The steps for\n",
    "        this algorithm are:\n",
    "            1. Run a forward pass of the model to get the activations \n",
    "               Corresponding to x and get the loss functionof the model \n",
    "               predictions compared to the target variable y\n",
    "            2. Compute the deltas (see lecture notes) and values of the\n",
    "               gradient with respect to each weight in each layer moving\n",
    "               backwards through the network\n",
    "    \n",
    "            Input:\n",
    "                x: A vector of 1 samples of data [n_in x 1]\n",
    "                y: Target variable [scalar]\n",
    "            Output:\n",
    "                loss: a scalar measure of th loss/cost associated with x,y\n",
    "                      and the current model weights\n",
    "        '''\n",
    "        \n",
    "    def stochastic_gradient_descent_step(self):\n",
    "        '''stochastic_gradient_descent_step\n",
    "        Using the gradient values computed by backpropagate, update each\n",
    "        weight value of the model according to the familiar stochastic\n",
    "        gradient descent update equation.\n",
    "        \n",
    "        Input: none\n",
    "        Output: none\n",
    "        '''\n",
    "    \n",
    "    def fit(self, X, y, max_epochs=, learning_rate=, get_validation_loss=):\n",
    "        '''fit\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "            Output:\n",
    "                training_loss:   Vector of training loss values at the end of each epoch\n",
    "                validation_loss: Vector of validation loss values at the end of each epoch\n",
    "                                 [optional output if get_validation_loss==True]\n",
    "        '''\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        '''predict_proba\n",
    "        Compute the output of the neural network for each sample in X, with the last layer's\n",
    "        sigmoid activation providing an estimate of the target output between 0 and 1\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions between 0 and 1 [N x 1]\n",
    "        '''\n",
    "    \n",
    "    def predict(self, X, decision_thresh=):\n",
    "        '''predict\n",
    "        Compute the output of the neural network prediction for \n",
    "        each sample in X, with the last layer's sigmoid activation \n",
    "        providing an estimate of the target output between 0 and 1, \n",
    "        then thresholding that prediction based on decision_thresh\n",
    "        to produce a binary class prediction\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                decision_threshold: threshold for the class confidence score\n",
    "                                    of predict_proba for binarizing the output\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions of either 0 or 1 [N x 1]\n",
    "        '''\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        '''sigmoid\n",
    "        Compute the sigmoid function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid function\n",
    "        '''\n",
    "    \n",
    "    def sigmoid_derivative(self, X):\n",
    "        '''sigmoid_derivative\n",
    "        Compute the sigmoid derivative function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid derivative function\n",
    "        '''\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "644px",
    "left": "1382px",
    "right": "20px",
    "top": "131px",
    "width": "367px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
